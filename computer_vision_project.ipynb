{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bd163a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (4.11.0.86)\n",
      "Collecting mediapipe\n",
      "  Downloading mediapipe-0.10.21-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Collecting yagmail\n",
      "  Downloading yagmail-0.15.293-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: absl-py in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from mediapipe) (25.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from mediapipe) (25.2.10)\n",
      "Collecting jax (from mediapipe)\n",
      "  Downloading jax-0.6.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jaxlib (from mediapipe)\n",
      "  Downloading jaxlib-0.6.1-cp312-cp312-win_amd64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from mediapipe) (3.10.0)\n",
      "Collecting opencv-contrib-python (from mediapipe)\n",
      "  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting protobuf<5,>=4.25.3 (from mediapipe)\n",
      "  Downloading protobuf-4.25.8-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
      "  Downloading sounddevice-0.5.2-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting sentencepiece (from mediapipe)\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl.metadata (8.3 kB)\n",
      "Collecting premailer (from yagmail)\n",
      "  Downloading premailer-3.10.0-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from jax->mediapipe) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.11.1 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from jax->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: lxml in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from premailer->yagmail) (5.2.1)\n",
      "Requirement already satisfied: cssselect in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from premailer->yagmail) (1.2.0)\n",
      "Collecting cssutils (from premailer->yagmail)\n",
      "  Downloading cssutils-2.11.1-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from premailer->yagmail) (2.32.3)\n",
      "Requirement already satisfied: cachetools in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from premailer->yagmail) (5.3.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from cssutils->premailer->yagmail) (10.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from requests->premailer->yagmail) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from requests->premailer->yagmail) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from requests->premailer->yagmail) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from requests->premailer->yagmail) (2025.1.31)\n",
      "Downloading mediapipe-0.10.21-cp312-cp312-win_amd64.whl (51.0 MB)\n",
      "   ---------------------------------------- 0.0/51.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/51.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 2.1/51.0 MB 9.0 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 12.3/51.0 MB 25.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 17.0/51.0 MB 28.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 23.6/51.0 MB 27.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 37.5/51.0 MB 34.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  50.9/51.0 MB 41.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 51.0/51.0 MB 39.1 MB/s eta 0:00:00\n",
      "Downloading yagmail-0.15.293-py2.py3-none-any.whl (17 kB)\n",
      "Downloading protobuf-4.25.8-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Downloading sounddevice-0.5.2-py3-none-win_amd64.whl (363 kB)\n",
      "Downloading jax-0.6.1-py3-none-any.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 142.2 MB/s eta 0:00:00\n",
      "Downloading jaxlib-0.6.1-cp312-cp312-win_amd64.whl (56.9 MB)\n",
      "   ---------------------------------------- 0.0/56.9 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 14.9/56.9 MB 72.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 25.7/56.9 MB 74.1 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 28.0/56.9 MB 45.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 30.7/56.9 MB 38.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 32.2/56.9 MB 31.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 45.4/56.9 MB 36.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.9 MB 40.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.9 MB 40.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.9 MB 40.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.9 MB 40.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.9 MB 40.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.9 MB 40.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.6/56.9 MB 40.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 56.9/56.9 MB 20.8 MB/s eta 0:00:00\n",
      "Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-win_amd64.whl (46.2 MB)\n",
      "   ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 14.7/46.2 MB 65.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 29.9/46.2 MB 70.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 38.8/46.2 MB 60.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 40.1/46.2 MB 48.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 41.9/46.2 MB 40.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  46.1/46.2 MB 39.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  46.1/46.2 MB 39.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.2/46.2 MB 28.0 MB/s eta 0:00:00\n",
      "Downloading premailer-3.10.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/992.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 992.0/992.0 kB 15.5 MB/s eta 0:00:00\n",
      "Downloading cssutils-2.11.1-py3-none-any.whl (385 kB)\n",
      "Installing collected packages: sentencepiece, protobuf, opencv-contrib-python, cssutils, sounddevice, premailer, jaxlib, yagmail, jax, mediapipe\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.26.1\n",
      "    Uninstalling protobuf-5.26.1:\n",
      "      Successfully uninstalled protobuf-5.26.1\n",
      "Successfully installed cssutils-2.11.1 jax-0.6.1 jaxlib-0.6.1 mediapipe-0.10.21 opencv-contrib-python-4.11.0.86 premailer-3.10.0 protobuf-4.25.8 sentencepiece-0.2.0 sounddevice-0.5.2 yagmail-0.15.293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ortools 9.11.4210 requires protobuf<5.27,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# !pip install opencv-python mediapipe yagmail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11af7509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.3.148-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: numpy>=1.23.0 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from ultralytics) (3.10.0)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from ultralytics) (10.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from ultralytics) (1.15.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from ultralytics) (2.6.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from ultralytics) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from ultralytics) (4.66.5)\n",
      "Requirement already satisfied: psutil in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from ultralytics) (2.2.3)\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
      "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
      "Requirement already satisfied: filelock in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\oklabo\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Downloading ultralytics-8.3.148-py3-none-any.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 0.8/1.0 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 5.6 MB/s eta 0:00:00\n",
      "Downloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: ultralytics-thop, ultralytics\n",
      "Successfully installed ultralytics-8.3.148 ultralytics-thop-2.0.14\n"
     ]
    }
   ],
   "source": [
    "# !pip install ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2856f35e",
   "metadata": {},
   "source": [
    "# Detection d'usage de telephone + envoie d'email avec capture de la preuve de cet incident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7412d19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Détection en cours... Appuyez sur Q pour quitter.\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 917.1ms\n",
      "Speed: 9.3ms preprocess, 917.1ms inference, 8.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 1267.9ms\n",
      "Speed: 11.0ms preprocess, 1267.9ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1036.9ms\n",
      "Speed: 5.0ms preprocess, 1036.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 853.0ms\n",
      "Speed: 2.6ms preprocess, 853.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 841.7ms\n",
      "Speed: 3.0ms preprocess, 841.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 897.5ms\n",
      "Speed: 3.6ms preprocess, 897.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1143.8ms\n",
      "Speed: 2.7ms preprocess, 1143.8ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 1187.7ms\n",
      "Speed: 4.0ms preprocess, 1187.7ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1076.1ms\n",
      "Speed: 3.4ms preprocess, 1076.1ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1036.7ms\n",
      "Speed: 3.6ms preprocess, 1036.7ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 975.3ms\n",
      "Speed: 3.4ms preprocess, 975.3ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 cravate, 972.7ms\n",
      "Speed: 2.7ms preprocess, 972.7ms inference, 7.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 cravate, 1 chaise, 952.7ms\n",
      "Speed: 3.4ms preprocess, 952.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 cravate, 926.8ms\n",
      "Speed: 3.1ms preprocess, 926.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 cravate, 1002.4ms\n",
      "Speed: 2.0ms preprocess, 1002.4ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 cravate, 1282.9ms\n",
      "Speed: 4.9ms preprocess, 1282.9ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 1105.6ms\n",
      "Speed: 3.3ms preprocess, 1105.6ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 cravate, 1134.6ms\n",
      "Speed: 4.4ms preprocess, 1134.6ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 cravate, 1131.6ms\n",
      "Speed: 4.4ms preprocess, 1131.6ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 cravate, 1062.5ms\n",
      "Speed: 4.7ms preprocess, 1062.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1149.0ms\n",
      "Speed: 3.5ms preprocess, 1149.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 857.1ms\n",
      "Speed: 3.5ms preprocess, 857.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 894.3ms\n",
      "Speed: 2.2ms preprocess, 894.3ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 878.0ms\n",
      "Speed: 3.2ms preprocess, 878.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 886.3ms\n",
      "Speed: 2.0ms preprocess, 886.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 915.0ms\n",
      "Speed: 4.0ms preprocess, 915.0ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 cravate, 1 chaise, 908.8ms\n",
      "Speed: 4.5ms preprocess, 908.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 830.2ms\n",
      "Speed: 2.9ms preprocess, 830.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 cravate, 1 chaise, 933.8ms\n",
      "Speed: 3.1ms preprocess, 933.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 cravate, 1 chaise, 1092.0ms\n",
      "Speed: 3.1ms preprocess, 1092.0ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 1172.4ms\n",
      "Speed: 4.3ms preprocess, 1172.4ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1110.1ms\n",
      "Speed: 3.9ms preprocess, 1110.1ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 cravate, 953.2ms\n",
      "Speed: 3.2ms preprocess, 953.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 cravate, 836.0ms\n",
      "Speed: 3.0ms preprocess, 836.0ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 849.7ms\n",
      "Speed: 2.0ms preprocess, 849.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 811.0ms\n",
      "Speed: 1.8ms preprocess, 811.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 cravate, 796.7ms\n",
      "Speed: 2.0ms preprocess, 796.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 810.9ms\n",
      "Speed: 3.2ms preprocess, 810.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 805.4ms\n",
      "Speed: 2.3ms preprocess, 805.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 cravate, 1 chaise, 850.5ms\n",
      "Speed: 1.6ms preprocess, 850.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 cravate, 1 chaise, 842.5ms\n",
      "Speed: 2.8ms preprocess, 842.5ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 cravate, 818.4ms\n",
      "Speed: 4.0ms preprocess, 818.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 795.6ms\n",
      "Speed: 2.4ms preprocess, 795.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 818.1ms\n",
      "Speed: 2.0ms preprocess, 818.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 tlphone portable, 799.2ms\n",
      "Ignoré téléphone portable : conf=0.58, taille=(127,18)\n",
      "Speed: 2.0ms preprocess, 799.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 cravate, 946.7ms\n",
      "Speed: 3.6ms preprocess, 946.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 1 tlphone portable, 1117.4ms\n",
      "Image envoyée à ka.ousmane@uam.edu.sn : capture_1749077280.jpg\n",
      "Speed: 2.2ms preprocess, 1117.4ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 cravate, 1 chaise, 1 tlphone portable, 657.9ms\n",
      "Speed: 1.2ms preprocess, 657.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 1 tlphone portable, 816.5ms\n",
      "Speed: 3.3ms preprocess, 816.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 tlphone portable, 833.7ms\n",
      "Speed: 2.1ms preprocess, 833.7ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 tlphone portable, 876.0ms\n",
      "Speed: 3.3ms preprocess, 876.0ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 tlphone portable, 939.0ms\n",
      "Speed: 3.1ms preprocess, 939.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 tlphone portable, 843.7ms\n",
      "Speed: 2.7ms preprocess, 843.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 tlphone portable, 880.6ms\n",
      "Speed: 3.3ms preprocess, 880.6ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 837.6ms\n",
      "Speed: 1.9ms preprocess, 837.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 861.0ms\n",
      "Speed: 4.8ms preprocess, 861.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 803.0ms\n",
      "Speed: 1.9ms preprocess, 803.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 947.2ms\n",
      "Speed: 2.4ms preprocess, 947.2ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1155.9ms\n",
      "Speed: 8.3ms preprocess, 1155.9ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 1280.3ms\n",
      "Speed: 4.0ms preprocess, 1280.3ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1247.5ms\n",
      "Speed: 4.6ms preprocess, 1247.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 983.5ms\n",
      "Speed: 7.5ms preprocess, 983.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 cravate, 1066.0ms\n",
      "Speed: 3.0ms preprocess, 1066.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1109.5ms\n",
      "Speed: 4.0ms preprocess, 1109.5ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1087.2ms\n",
      "Speed: 3.6ms preprocess, 1087.2ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1054.9ms\n",
      "Speed: 3.4ms preprocess, 1054.9ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1097.9ms\n",
      "Speed: 3.9ms preprocess, 1097.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1083.9ms\n",
      "Speed: 4.5ms preprocess, 1083.9ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1088.3ms\n",
      "Speed: 3.3ms preprocess, 1088.3ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1092.5ms\n",
      "Speed: 3.3ms preprocess, 1092.5ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 1054.8ms\n",
      "Speed: 2.8ms preprocess, 1054.8ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1348.0ms\n",
      "Speed: 4.4ms preprocess, 1348.0ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 1496.6ms\n",
      "Speed: 4.5ms preprocess, 1496.6ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 1403.6ms\n",
      "Speed: 3.9ms preprocess, 1403.6ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 1240.1ms\n",
      "Speed: 3.9ms preprocess, 1240.1ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1318.4ms\n",
      "Speed: 3.4ms preprocess, 1318.4ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 1280.1ms\n",
      "Speed: 4.6ms preprocess, 1280.1ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 1201.6ms\n",
      "Speed: 3.2ms preprocess, 1201.6ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1167.7ms\n",
      "Speed: 4.0ms preprocess, 1167.7ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1282.1ms\n",
      "Speed: 4.9ms preprocess, 1282.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1283.4ms\n",
      "Speed: 3.4ms preprocess, 1283.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 1175.8ms\n",
      "Speed: 2.9ms preprocess, 1175.8ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1315.0ms\n",
      "Speed: 3.7ms preprocess, 1315.0ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 personne, 1 chaise, 1370.8ms\n",
      "Speed: 4.7ms preprocess, 1370.8ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 56\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     54\u001b[0m resultats \u001b[38;5;241m=\u001b[39m modele(frame, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, conf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m resultat \u001b[38;5;129;01min\u001b[39;00m resultats:\n\u001b[0;32m     57\u001b[0m     personnes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     58\u001b[0m     telephones \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:330\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 330\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:182\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    177\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    178\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    181\u001b[0m )\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:636\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed, **kwargs)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[1;32m--> 636\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39membed, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    638\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:138\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:156\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_once(x, profile, visualize, embed)\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:179\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 179\u001b[0m x \u001b[38;5;241m=\u001b[39m m(x)  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    180\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:318\u001b[0m, in \u001b[0;36mC2f.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[0;32m    317\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 318\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:318\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[0;32m    317\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 318\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:495\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    494\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply bottleneck with optional shortcut connection.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x))\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:92\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     83\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    Apply convolution and activation without batch normalization.\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x))\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    551\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import yagmail # Pour envoyer des e-mails facilement depuis Python\n",
    "import time\n",
    "\n",
    "# Liste traduite des noms COCO\n",
    "noms_classes_fr = [\n",
    "    \"personne\", \"vélo\", \"voiture\", \"moto\", \"avion\", \"bus\", \"train\", \"camion\",\n",
    "    \"bateau\", \"feu de signalisation\", \"borne d'incendie\", \"panneau d'arrêt\", \"parcmètre\", \"banc\",\n",
    "    \"oiseau\", \"chat\", \"chien\", \"cheval\", \"mouton\", \"vache\", \"éléphant\", \"ours\", \"zèbre\", \"girafe\",\n",
    "    \"sac à dos\", \"parapluie\", \"sac à main\", \"cravate\", \"valise\", \"frisbee\", \"skis\", \"snowboard\",\n",
    "    \"ballon de sport\", \"cerf-volant\", \"batte de baseball\", \"gant de baseball\", \"skateboard\", \"planche de surf\",\n",
    "    \"raquette de tennis\", \"bouteille\", \"verre à vin\", \"tasse\", \"fourchette\", \"couteau\", \"cuillère\", \"bol\", \"banane\",\n",
    "    \"pomme\", \"sandwich\", \"orange\", \"brocoli\", \"carotte\", \"hot-dog\", \"pizza\", \"beignet\", \"gâteau\",\n",
    "    \"chaise\", \"canapé\", \"plante en pot\", \"lit\", \"table à manger\", \"toilettes\", \"téléviseur\", \"ordinateur portable\", \"souris\",\n",
    "    \"télécommande\", \"clavier\", \"téléphone portable\", \"micro-ondes\", \"four\", \"grille-pain\", \"évier\", \"réfrigérateur\",\n",
    "    \"livre\", \"horloge\", \"vase\", \"ciseaux\", \"ours en peluche\", \"sèche-cheveux\", \"brosse à dents\"\n",
    "]\n",
    "\n",
    "# Chargement du modèle YOLOv8\n",
    "modele = YOLO(\"yolov8l.pt\")\n",
    "modele.model.names = {i: nom for i, nom in enumerate(noms_classes_fr)}\n",
    "\n",
    "# Initialisation de la webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 640) # Largeur \n",
    "cap.set(4, 480) # Hauteur\n",
    "\n",
    "# Configuration de l'envoi d'e-mail\n",
    "EMAIL_SENDER = 'ka.ousmane@uam.edu.sn'\n",
    "EMAIL_PASSWORD = 'ccba qcup mbwb jyra'\n",
    "EMAIL_RECEIVER = 'ka.ousmane@uam.edu.sn'\n",
    "yag = yagmail.SMTP(EMAIL_SENDER, EMAIL_PASSWORD)\n",
    "\n",
    "# Variable de contrôle d'envoi pour éviter d’envoyer plusieurs e-mails pour la même détection\n",
    "image_envoyee = False\n",
    "\n",
    "# Vérifie si deux boîtes sont proches (personne et telephone portable)\n",
    "def sont_proches(box1, box2, marge=50):\n",
    "    x1, y1, x2, y2 = box1\n",
    "    a1, b1, a2, b2 = box2\n",
    "    return not (x2 + marge < a1 or x1 - marge > a2 or y2 + marge < b1 or y1 - marge > b2)\n",
    "\n",
    "print(\"Détection en cours... Appuyez sur Q pour quitter.\")\n",
    "\n",
    "SEUIL_TELEPHONE = 0.75   # seuil plus strict\n",
    "TAILLE_MINI_OBJET = 40   # taille minimale pour considérer un objet\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print(\"Erreur de lecture du flux vidéo.\")\n",
    "        break\n",
    "\n",
    "    resultats = modele(frame, stream=True, conf=0.5)\n",
    "\n",
    "    for resultat in resultats:\n",
    "        personnes = []\n",
    "        telephones = []\n",
    "\n",
    "        for box in resultat.boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            nom_classe = noms_classes_fr[cls_id]\n",
    "            conf = float(box.conf[0])\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            largeur = x2 - x1\n",
    "            hauteur = y2 - y1\n",
    "\n",
    "            # Filtrage selon les classes détectées\n",
    "            if nom_classe == \"personne\":\n",
    "                personnes.append((x1, y1, x2, y2))\n",
    "                # Si la classe détectée est “personne”, on l’ajoute à la liste.\n",
    "\n",
    "            elif nom_classe == \"téléphone portable\":\n",
    "                if conf >= SEUIL_TELEPHONE and largeur > TAILLE_MINI_OBJET and hauteur > TAILLE_MINI_OBJET:\n",
    "                    telephones.append((x1, y1, x2, y2))\n",
    "                    # Si c’est un téléphone avec confiance suffisante et taille réaliste, on le garde aussi.\n",
    "                else:\n",
    "                    print(f\"Ignoré {nom_classe} : conf={conf:.2f}, taille=({largeur},{hauteur})\")\n",
    "\n",
    "        # vérifie s’il y a une proximité spatiale entre une personne et un téléphone.\n",
    "        for personne in personnes:\n",
    "            for tel in telephones:\n",
    "                if sont_proches(personne, tel) and not image_envoyee:\n",
    "                    nom_fichier = f\"capture_{int(time.time())}.jpg\"\n",
    "                    cv2.imwrite(nom_fichier, frame)\n",
    "                    # enregistre l’image avec un nom unique basé sur l’heure actuelle\n",
    "                    yag.send(\n",
    "                        to=EMAIL_RECEIVER,\n",
    "                        subject=\"Téléphone détecté avec une personne\",\n",
    "                        contents=\"Une personne a été détectée en train d'utiliser un téléphone.\",\n",
    "                        attachments=nom_fichier\n",
    "                    )\n",
    "                    print(f\"Image envoyée à {EMAIL_RECEIVER} : {nom_fichier}\")\n",
    "                    image_envoyee = True\n",
    "\n",
    "        image_annotée = resultat.plot()\n",
    "        cv2.imshow(\"Détection - YOLOv8 (FR)\", image_annotée)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcf7fc9",
   "metadata": {},
   "source": [
    "# Ajout de filtre avec lunette et chapeau en tenant compte de l'inclinaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd7e500",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m h, w \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     51\u001b[0m rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 52\u001b[0m resultats \u001b[38;5;241m=\u001b[39m face_mesh\u001b[38;5;241m.\u001b[39mprocess(rgb)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resultats\u001b[38;5;241m.\u001b[39mmulti_face_landmarks:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m visage \u001b[38;5;129;01min\u001b[39;00m resultats\u001b[38;5;241m.\u001b[39mmulti_face_landmarks:\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\mediapipe\\python\\solutions\\face_mesh.py:125\u001b[0m, in \u001b[0;36mFaceMesh.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    111\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the face landmarks on each detected face.\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m    face landmarks on each detected face.\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mprocess(input_data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: image})\n",
      "File \u001b[1;32mc:\\Users\\oklabo\\anaconda3\\Lib\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mwait_until_idle()\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "mp_face = mp.solutions.face_mesh\n",
    "face_mesh = mp_face.FaceMesh(refine_landmarks=True)\n",
    "mp_draw = mp.solutions.drawing_utils  # Pour afficher les points détectés\n",
    "\n",
    "# Chargement des images (avec transparence, canal alpha)\n",
    "lunette = cv2.imread(\"lunette.png\", cv2.IMREAD_UNCHANGED)\n",
    "chapeau = cv2.imread(\"chapeau.png\", cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# Fonction pour coller une image PNG avec rotation et transparence sur l’image de la webcam\n",
    "def superposer_image(image, filtre, centre, taille, angle_deg):\n",
    "    # image = image du webcam\n",
    "    # filtre = image du filtre (lunette ou chapeau)\n",
    "    # centre = position du filtre (centre des yeux ou tête)\n",
    "    # taille = (largeur, hauteur) du filtre\n",
    "    # angle_deg = angle de rotation du filtre\n",
    "\n",
    "    filtre = cv2.resize(filtre, taille) # Redimensionne le filtre selon la taille souhaitée\n",
    "    M = cv2.getRotationMatrix2D((taille[0] // 2, taille[1] // 2), angle_deg, 1) # Rotation autour du centre\n",
    "    filtre = cv2.warpAffine(filtre, M, taille, borderValue=(0, 0, 0, 0)) # Applique la rotation au filtre\n",
    "\n",
    "    # séparation des canaux couleur + alpha\n",
    "    b, g, r, a = cv2.split(filtre)\n",
    "    masque = cv2.merge((a, a, a))  # utilisé comme masque de transparence\n",
    "    couleur = cv2.merge((b, g, r)) # image sans alpha\n",
    "\n",
    "    # Définition de la zone sur laquelle superposer\n",
    "    x, y = centre[0] - taille[0] // 2, centre[1] - taille[1] // 2\n",
    "    x1, y1 = max(x, 0), max(y, 0)\n",
    "    x2, y2 = min(x + taille[0], image.shape[1]), min(y + taille[1], image.shape[0])\n",
    "\n",
    "    roi = image[y1:y2, x1:x2] # ROI : zone de collage\n",
    "    masque_crop = masque[y1 - y:y2 - y, x1 - x:x2 - x]\n",
    "    couleur_crop = couleur[y1 - y:y2 - y, x1 - x:x2 - x]\n",
    "\n",
    "    # Application du filtre sur le fond selon la transparence\n",
    "    image[y1:y2, x1:x2] = np.where(masque_crop == 0, roi, couleur_crop)\n",
    "    return image\n",
    "\n",
    "# Calcule l’angle d’inclinaison latérale (gauche/droite) entre les yeux\n",
    "def calcul_roll(p1, p2):\n",
    "    dx = p2[0] - p1[0]\n",
    "    dy = p2[1] - p1[1]\n",
    "    return math.degrees(math.atan2(dy, dx))\n",
    "\n",
    "# Calcule l’inclinaison verticale (haut/bas) entre les yeux et le menton\n",
    "def calcul_pitch(p1, p2):\n",
    "    dx = p2[0] - p1[0]\n",
    "    dy = p2[1] - p1[1]\n",
    "    return abs(math.degrees(math.atan2(dy, dx)))\n",
    "\n",
    "# Activation de la webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break # Si aucun flux n’est lu, on quitte\n",
    "\n",
    "    h, w = frame.shape[:2] # Dimensions de l’image\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # Conversion en RGB pour MediaPipe\n",
    "    resultats = face_mesh.process(rgb) # Détection des landmarks du visage\n",
    "\n",
    "    if resultats.multi_face_landmarks:\n",
    "        for visage in resultats.multi_face_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, visage, mp_face.FACEMESH_CONTOURS)\n",
    "\n",
    "            # Points clés\n",
    "            oeil_g = visage.landmark[33]\n",
    "            oeil_d = visage.landmark[263]\n",
    "            menton = visage.landmark[152]\n",
    "\n",
    "            # Conversion des coordonnées en pixels\n",
    "            xg, yg = int(oeil_g.x * w), int(oeil_g.y * h)\n",
    "            xd, yd = int(oeil_d.x * w), int(oeil_d.y * h)\n",
    "            xm, ym = int(menton.x * w), int(menton.y * h)\n",
    "\n",
    "            centre_yeux = ((xg + xd) // 2, (yg + yd) // 2)\n",
    "\n",
    "            # Distance visage verticale (yeux → menton)\n",
    "            distance_face = ym - centre_yeux[1]\n",
    "\n",
    "            # Placement lunettes - calcul de la largeur entre les yeux\n",
    "            largeur_yeux = int(math.hypot(xd - xg, yd - yg))\n",
    "            largeur_lunette = largeur_yeux * 2 # Taille des lunettes proportionnelle\n",
    "\n",
    "            # Placement chapeau : au-dessus des yeux selon distance du visage\n",
    "            chapeau_y = int(centre_yeux[1] - 1.2 * distance_face)\n",
    "            decalage_x = int(roll * largeur_yeux * 0.02) # Ajustement horizontal du chapeau selon l’inclinaison gauche/droite\n",
    "            # centre_chapeau = (centre_yeux[0], chapeau_y)\n",
    "            centre_chapeau = (centre_yeux[0] + decalage_x, chapeau_y)\n",
    "            largeur_chapeau = largeur_yeux * 3 # Taille du chapeau\n",
    "\n",
    "            # Calcul des angles d’inclinaison\n",
    "            roll = calcul_roll((xg, yg), (xd, yd))\n",
    "            pitch = calcul_pitch((centre_yeux[0], centre_yeux[1]), (xm, ym))\n",
    "\n",
    "            # Affichage de l’angle vertical\n",
    "            cv2.putText(frame, f\"Pitch: {pitch:.1f}°\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
    "\n",
    "            # Message en fonction de l'inclinaison détectée\n",
    "            msg = \"Tête alignée\"\n",
    "            if pitch < 80: msg = \"Tête vers le BAS\"\n",
    "            elif pitch > 100: msg = \"Tête vers le HAUT\"\n",
    "            cv2.putText(frame, msg, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7,\n",
    "                        (0, 255, 0) if \"alignée\" in msg else (0, 0, 255), 2)\n",
    "\n",
    "            # Superposition des filtres dynamiques (lunettes et chapeau)\n",
    "            frame = superposer_image(frame, lunette, centre_yeux, (largeur_lunette, largeur_yeux), -roll)\n",
    "            frame = superposer_image(frame, chapeau, centre_chapeau, (largeur_chapeau, largeur_yeux * 2), -roll)\n",
    "    \n",
    "    # Affichage de l’image finale\n",
    "    cv2.imshow(\"Chapeau parfait (position naturelle)\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "    \n",
    "# Fermeture propre de la webcam\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
